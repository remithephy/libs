{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "'''\n",
    ":@Author: Remi\n",
    ":@Date: 2023/9/13 08:21:53\n",
    ":@LastEditors: Remi\n",
    ":@LastEditTime: 2024/3/17 20:25:23\n",
    ":Description: \n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "import preprocessing as pp\n",
    "import file_tool as ft\n",
    "\n",
    "path = r'D:\\20240412\\origin'\n",
    "path_list = ft.find_file(path,'.csv')\n",
    "\n",
    "filename = [os.path.basename(path_list[i])[:-4] for i in range(len(path_list))]####保留文件名\n",
    "\n",
    "\n",
    "#################default_iter##############\n",
    "aim_peak = []\n",
    "scores = 1\n",
    "#####ctrl ku  ctrl kc\n",
    "\n",
    "################################内标用到的峰#######################################\n",
    "wl,spec = ft.read_ava(path_list[0])\n",
    "maxspec = np.max(spec,axis = 1)\n",
    "int_peak = [742.361,744.322,746.852]##氢线656.302      氮线742.361,744.322,746.852     氧777.257\n",
    "peak_para = pp.Peak_integrate(wl,maxspec,int_peak)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "specs = []\n",
    "for path_num,paths in enumerate(path_list):\n",
    "    wl,spec = ft.read_ava(paths)\n",
    "    mean_spec = np.mean(spec,axis = 1)\n",
    "    specs.append(np.hstack((int(filename[path_num]),mean_spec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "wl = np.hstack((np.array(0).reshape(-1),wl.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "a = np.vstack((wl.reshape(-1),np.array(specs))).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.savetxt(path + '//' + 'origin.csv',a,delimiter=',',fmt = '%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for path_num,paths in enumerate(path_list):\n",
    "    method = 'total_intensity'#'total_intensity' 'internal_standard'\n",
    "    print('正在计算：' + paths,'method = ' + method)\n",
    "    wl,spec = ft.read_ava(paths)\n",
    "    rsd = []\n",
    "\n",
    "    channel = 0###########10381最后一个通道，8284倒数第二个spec[8284:,:],进行切片###########\n",
    "\n",
    "    spec = pp.Anomalous_spectrum_removal(spec[channel:,:])\n",
    "    wl = wl[channel:]\n",
    "    spec_num = len(spec[0,:])\n",
    "    \n",
    "    rsd.append(pp.RSD_calculate(spec,spec_num))\n",
    "    #导入对应一个波长的所有光谱数据，平均将光谱分为n份的数量\n",
    "\n",
    "\n",
    "    ##########weighted method\n",
    "    ##############genrate slope map#########\n",
    "    ###################feather devide###########    \n",
    "    for i in range(spec_num):#calculate slope\n",
    "        \n",
    "        spec[:,i] = pp.Normalization(spec[:,i],method,peak_para) \n",
    "        \n",
    "#########################slope 计算部分#########################################  \n",
    "        single_spec = spec[:,i]\n",
    "        slope = np.int64((np.append(single_spec,single_spec[-1]) - np.insert(single_spec,0,single_spec[0]))>= 0)#eg  0123 vs 0 -1 1 1 0 ,-1 is the slope between num0&1\n",
    "        peak = np.insert(slope,0,slope[0]) - np.append(slope,slope[-1])\n",
    "        if i == 0:\n",
    "            slope_map = slope\n",
    "            peak_map = peak\n",
    "        else:\n",
    "            slope_map = np.vstack((slope_map,slope)) \n",
    "            peak_map = np.vstack((peak_map,peak)) \n",
    "            #map 1 = + while 0 = -\n",
    "    \n",
    "    slope_map[slope_map == 0] = - 1\n",
    "    peak_map = peak_map[:,1:-1].T###计算peak形状\n",
    "\n",
    "    for single_ in slope_map:###计算 arise 和 decay 形状\n",
    "        for i in range(len(single_)-1):\n",
    "            if single_[i] != single_[i+1]:\n",
    "                single_[i] = 0\n",
    "\n",
    "    scores =  (np.abs(np.sum(peak_map,axis = 1)) +  np.abs(np.sum(slope_map[:,:-1],axis = 0)))/spec_num\n",
    "###################################################################################\n",
    "\n",
    "    mean_spec = np.mean(spec,axis = 1)\n",
    "    wei_spec = (spec.T * scores).T\n",
    "\n",
    "###########other process method\n",
    "    '''\n",
    "    different weighed method\n",
    "    '''\n",
    "    #weimean_spec = mean_spec * np.log((scores * (math.e - 1) + 1))\n",
    "    weimean_spec = mean_spec * scores\n",
    "\n",
    "    #slice_wl,slice_spec = pp.spec_slice([50,50],[50,50],[11544,11644],wl,spec)\n",
    "    #slice_wl = slice_wl.flatten()\n",
    "    weimean_spec = pp.Normalization(weimean_spec,method,peak_para) \n",
    "    \n",
    "    #np.savetxt(path + '//' +filename[path_num]+ 'wei_nm.csv',np.hstack((wl,wei_spec)),delimiter=',',fmt = '%s')\n",
    "    np.savetxt(path + '//' +filename[path_num]+ 'nm.csv',np.hstack((wl,spec)),delimiter=',',fmt = '%s')\n",
    "\n",
    "#########for save\n",
    "    if path_num == 0:\n",
    "        wl_list = np.hstack((np.array(([['wl']])).T,wl.T))\n",
    "        mean_spec_list = np.hstack((filename[path_num],mean_spec))\n",
    "    else:\n",
    "        mean_spec_list = np.vstack((mean_spec_list,np.hstack((filename[path_num],mean_spec))))\n",
    "    \n",
    "    if path_num == 0:\n",
    "        wl_list = np.hstack((np.array(([['wl']])).T,wl.T))\n",
    "        wei_spec_list = np.hstack((filename[path_num],weimean_spec))\n",
    "    else:\n",
    "        wei_spec_list = np.vstack((wei_spec_list,np.hstack((filename[path_num],weimean_spec))))\n",
    "\n",
    "#np.savetxt(path + '//' +filename[path_num]+ 'sliced.csv',np.hstack((np.array(([slice_wl])).T,slice_spec)),delimiter=',',fmt='%.04f')\n",
    "#np.savetxt(path + '//' +filename[path_num]+ 'wei.csv',np.hstack((wl,np.vstack((mean_spec,weimean_spec)).T)),delimiter=',',fmt='%.04f')\n",
    "#np.savetxt(path + '//' + 'nm.csv',np.vstack((wl_list,mean_spec_list)).T,delimiter=',',fmt = '%s')\n",
    "\n",
    "#np.savetxt(path + '//' + 'internal_nm.csv',np.vstack((wl_list,mean_spec_list)).T,delimiter=',',fmt = '%s')\n",
    "#np.savetxt(path + '//' + 'internal_wei.csv',np.vstack((wl_list,wei_spec_list)).T,delimiter=',',fmt = '%s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
