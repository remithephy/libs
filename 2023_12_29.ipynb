{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在计算：D:\\20231218\\origin\\15\\15.csv method = internal_standard\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    ":@Author: Remi\n",
    ":@Date: 2023/9/13 08:21:53\n",
    ":@LastEditors: Remi\n",
    ":@LastEditTime: 2023/12/29 16:10:52\n",
    ":Description: \n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "import preprocessing as pp\n",
    "import file_tool as ft\n",
    "\n",
    "path = r'D:\\20231218\\origin'\n",
    "path_list = ft.find_file(path,'.csv')\n",
    "\n",
    "filename = [os.path.basename(path_list[i])[:-4] for i in range(len(path_list))]####保留文件名\n",
    "\n",
    "\n",
    "#################default_iter##############\n",
    "aim_peak = []\n",
    "scores = 1\n",
    "#####ctrl ku  ctrl kc\n",
    "\n",
    "################################内标用到的峰#######################################\n",
    "wl,spec = ft.read_ava(path_list[0])\n",
    "maxspec = np.max(spec,axis = 1)\n",
    "int_peak = [742.361,744.322,746.852]##氢线656.302      氮线742.361,744.322,746.852     氧777.257\n",
    "peak_para = pp.Peak_integrate(wl,maxspec,int_peak)\n",
    "\n",
    "\n",
    "\n",
    "for path_num,paths in enumerate(path_list[0:1]):\n",
    "    method = 'internal_standard'#'total_intensity' 'internal_standard'\n",
    "    print('正在计算：' + paths,'method = ' + method)\n",
    "    wl,spec = ft.read_ava(paths)\n",
    "    rsd = []\n",
    "    channel = 0###########10381最后一个通道，8284倒数第二个spec[8284:,:]###########\n",
    "    spec = pp.Anomalous_spectrum_removal(spec[channel:,:])\n",
    "    wl = wl[channel:]#################################################################切片了，不切的删掉这行\n",
    "    spec_num = len(spec[0,:])\n",
    "    \n",
    "    rsd.append(pp.RSD_calculate(spec,spec_num))\n",
    "    #导入对应一个波长的所有光谱数据，平均将光谱分为n份的数量\n",
    "\n",
    "\n",
    "    ##########weighted method\n",
    "    ##############genrate slope map#########\n",
    "    ###################feather devide###########    \n",
    "    for i in range(spec_num):#calculate slope\n",
    "        \n",
    "        spec[:,i] = pp.Normalization(spec[:,i],method,peak_para) \n",
    "        \n",
    "#########################slope 计算部分#########################################  \n",
    "        single_spec = spec[:,i]\n",
    "        slope = np.int64((np.append(single_spec,single_spec[-1]) - np.insert(single_spec,0,single_spec[0]))>= 0)#eg  0123 vs 0 -1 1 1 0 ,-1 is the slope between num0&1\n",
    "        peak = np.insert(slope,0,slope[0]) - np.append(slope,slope[-1])\n",
    "        if i == 0:\n",
    "            slope_map = slope\n",
    "            peak_map = peak\n",
    "        else:\n",
    "            slope_map = np.vstack((slope_map,slope)) \n",
    "            peak_map = np.vstack((peak_map,peak)) \n",
    "            #map 1 = + while 0 = -\n",
    "    \n",
    "    slope_map[slope_map == 0] = - 1\n",
    "    peak_map = peak_map[:,1:-1].T###计算peak形状\n",
    "\n",
    "    for single_ in slope_map:###计算 arise 和 decay 形状\n",
    "        for i in range(len(single_)-1):\n",
    "            if single_[i] != single_[i+1]:\n",
    "                single_[i] = 0\n",
    "\n",
    "    scores =  (np.abs(np.sum(peak_map,axis = 1)) +  np.abs(np.sum(slope_map[:,:-1],axis = 0)))/spec_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei_spec = (spec.T * scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.78332000e+02, 1.78376000e+02, 1.78420000e+02, ...,\n",
       "        9.15392000e+02, 9.15468000e+02, 9.15544000e+02],\n",
       "       [2.25451325e-02, 1.04085781e-02, 1.65963264e-02, ...,\n",
       "        2.06749181e-02, 2.13517856e-02, 2.56899452e-02],\n",
       "       [2.39369690e-02, 1.11545643e-02, 1.80475035e-02, ...,\n",
       "        2.14611070e-02, 2.27918112e-02, 2.63979258e-02],\n",
       "       ...,\n",
       "       [3.69198895e-02, 1.58396182e-02, 2.66802963e-02, ...,\n",
       "        3.48121772e-02, 2.83264429e-02, 4.32152447e-02],\n",
       "       [3.47799514e-02, 1.82326672e-02, 2.68777255e-02, ...,\n",
       "        3.75954344e-02, 3.59701879e-02, 4.51755182e-02],\n",
       "       [4.51039447e-02, 2.31169664e-02, 3.94401085e-02, ...,\n",
       "        4.75176316e-02, 4.45511950e-02, 6.49103710e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((wl,wei_spec)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(path + '//' + 'wei_nm.csv',np.hstack((wl,wei_spec)),delimiter=',',fmt = '%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "\n",
    "    mean_spec = np.mean(spec,axis = 1)\n",
    "\n",
    "###########other process method\n",
    "    '''\n",
    "    different weighed method\n",
    "    '''\n",
    "    #weighter_spec = mean_spec * np.log((scores * (math.e - 1) + 1))\n",
    "    weighter_spec = mean_spec * scores\n",
    "\n",
    "    #slice_wl,slice_spec = pp.spec_slice([50,50],[50,50],[11544,11644],wl,spec)\n",
    "    #slice_wl = slice_wl.flatten()\n",
    "    weighter_spec = pp.Normalization(weighter_spec,method,peak_para) \n",
    "\n",
    "\n",
    "#########for save\n",
    "    if path_num == 0:\n",
    "        wl_list = np.hstack((np.array(([['wl']])).T,wl.T))\n",
    "        mean_spec_list = np.hstack((filename[path_num],mean_spec))\n",
    "    else:\n",
    "        mean_spec_list = np.vstack((mean_spec_list,np.hstack((filename[path_num],mean_spec))))\n",
    "    \n",
    "    if path_num == 0:\n",
    "        wl_list = np.hstack((np.array(([['wl']])).T,wl.T))\n",
    "        wei_spec_list = np.hstack((filename[path_num],weighter_spec))\n",
    "    else:\n",
    "        wei_spec_list = np.vstack((wei_spec_list,np.hstack((filename[path_num],weighter_spec))))\n",
    "\n",
    "#np.savetxt(path + '//' +filename[path_num]+ 'sliced.csv',np.hstack((np.array(([slice_wl])).T,slice_spec)),delimiter=',',fmt='%.04f')\n",
    "#np.savetxt(path + '//' +filename[path_num]+ 'wei.csv',np.hstack((wl,np.vstack((mean_spec,weighter_spec)).T)),delimiter=',',fmt='%.04f')\n",
    "#np.savetxt(path + '//' + 'nm.csv',np.vstack((wl_list,mean_spec_list)).T,delimiter=',',fmt = '%s')\n",
    "np.savetxt(path + '//' + 'internal_nm.csv',np.vstack((wl_list,mean_spec_list)).T,delimiter=',',fmt = '%s')\n",
    "np.savetxt(path + '//' + 'internal_wei.csv',np.vstack((wl_list,wei_spec_list)).T,delimiter=',',fmt = '%s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ten",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
