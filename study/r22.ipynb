{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import perturbation,CNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "\n",
    "path = r'D:\\20240412\\wei\\all.csv'\n",
    "label_path = r'D:\\20240412\\alllab.csv'\n",
    "wl_path = r'D:\\20240412\\wl.csv'\n",
    "save_path = r'D:\\20240412\\wei\\cnn'\n",
    "\n",
    "# super iter\n",
    "num_feat = 10\n",
    "random_seed = 2024  \n",
    "batch_size = 100\n",
    "lr = 5e-2\n",
    "num_epochs = 800\n",
    "step_epoch = 1\n",
    "new_feat_size = 300\n",
    "mean_times = 50\n",
    "new_feat = []\n",
    "new_lab = []\n",
    "\n",
    "######### read raw data ####################\n",
    "raw_feat = pd.read_csv(path,header=None).values.astype(np.float32)[:,12195-4096:12195]\n",
    "raw_label = pd.read_csv(label_path,header=None).values.astype(np.float32)\n",
    "raw_wl = pd.read_csv(wl_path,header=None).values.astype(np.float32)[12201-4096:12201]\n",
    "\n",
    "######### make new feat ####################\n",
    "unique_lab = np.unique(raw_label)\n",
    "for labs in unique_lab:\n",
    "    index = np.isin(raw_label,labs).reshape(-1)\n",
    "    unique_feat = raw_feat[index,:]\n",
    "    for _ in range(new_feat_size):\n",
    "        new_feat.append(np.mean(unique_feat[sample(range(0,len(unique_feat[:,0]-1)),mean_times),:],axis = 0))\n",
    "        new_lab.append(labs)\n",
    "\n",
    "new_feat = np.array(new_feat)\n",
    "new_lab = np.array(new_lab).reshape(-1,1)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# new_feat = scaler.fit_transform(new_feat.T).T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 取测试集\n",
    "unique_lab = np.unique(new_lab)\n",
    "\n",
    "select_lab = unique_lab[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "norm_feat = []\n",
    "\n",
    "\n",
    "test_index = np.isin(new_lab,select_lab).reshape(-1)#-600 500 -1 50 \n",
    "\n",
    "# # normalization\n",
    "# feat_scaler = MinMaxScaler()\n",
    "# lab_scaler = MinMaxScaler()\n",
    "\n",
    "# for spec in range(len(new_feat[:,0])):\n",
    "#     norm_feat.append(prep.Normalization(new_feat[spec,:]) )\n",
    "# norm_feat = np.array(norm_feat) * 1000\n",
    "norm_feat = new_feat*1000\n",
    "# norm_lab = lab_scaler.fit_transform(raw_label)\n",
    "\n",
    "#buyong scaler\n",
    "\n",
    "test_feat = norm_feat[test_index,:]\n",
    "test_label = new_lab[test_index]\n",
    "train_feat = norm_feat[~test_index,:]\n",
    "train_label = new_lab[~test_index]\n",
    "\n",
    "# random seeding\n",
    "\n",
    "np.random.seed(random_seed)  \n",
    "random_index = [i for i in range(len(train_label))]\n",
    "np.random.shuffle(random_index)\n",
    "\n",
    "train_feat = train_feat[random_index,:]\n",
    "train_label = train_label[random_index]\n",
    "\n",
    "\n",
    "# # import select\n",
    "index_feat = pd.read_csv(r'D:\\20240412\\select_wl.csv',header=None).values[:,1].astype(int)\n",
    "index_feat = index_feat + 2048\n",
    "'CNN不用index'\n",
    "# train_feat = train_feat[:, index_feat]\n",
    "# test_feat = test_feat[:, index_feat]\n",
    "\n",
    "select_wl = raw_wl[index_feat]\n",
    "\n",
    "# split train and test\n",
    "# x_train,x_test,y_train,y_test = train_test_split(\n",
    "#     norm_feat,norm_lab, test_size=0.2,shuffle = False)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_feat,test_feat,train_label,test_label\n",
    "\n",
    "\n",
    "\n",
    "train_data = torch.tensor(x_train, dtype=torch.float32)\n",
    "train_label = torch.tensor(y_train, dtype=torch.float32)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(train_data, train_label), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = torch.tensor(x_test, dtype=torch.float32)\n",
    "test_label = torch.tensor(y_test, dtype=torch.float32)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(test_data, test_label), batch_size=batch_size, shuffle=True)\n",
    "'''remember to set shuffle to False'''\n",
    "\n",
    "\n",
    "# parameter define\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"Using GPU !\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "logdir = os.getcwd()\n",
    "device = 'cpu'\n",
    "path = r'D:\\20240412\\wei\\cnn' + '\\\\' + str(select_lab) + 'best_net.pth'\n",
    "model = torch.load(path)\n",
    "perturb = perturbation(4096)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "perturb_optimizer = torch.optim.Adam(perturb.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variable define\n",
    "all_sigma = []\n",
    "y_train1 = []\n",
    "y_test1 = []\n",
    "y_train2 = []\n",
    "y_test2 = []\n",
    "## 模型训练\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     sum_train_r2 = 0\n",
    "#     sum_test_r2 = 0\n",
    "#     for x, y in train_dataloader:\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "#         preds = model(x)\n",
    "\n",
    "#         # 评估指标\n",
    "#         # y_denormalized = lab_scaler.inverse_transform(y.cpu())\n",
    "#         # preds_denormalized = lab_scaler.inverse_transform(preds.cpu())\n",
    "#         y_denormalized = y\n",
    "#         preds_denormalized = preds\n",
    "#         sum_train_r2 = sum_train_r2 + r2_score(y_denormalized,preds_denormalized)\n",
    "#         y_train1.append(preds_denormalized.numpy().reshape(-1,1))\n",
    "#         y_train2.append(y_denormalized.numpy().reshape(-1,1))\n",
    "#         # 计算损失\n",
    "#     for x_t, y_t in test_dataloader:\n",
    "#         x_t = x_t.to(device)\n",
    "#         y_t = y_t.to(device)\n",
    "#         preds = model(x_t)\n",
    "\n",
    "#         # 评估指标\n",
    "#         # y_denormalized = lab_scaler.inverse_transform(y_t.cpu())\n",
    "#         # preds_denormalized = lab_scaler.inverse_transform(preds.cpu())\n",
    "#         y_denormalized = y_t\n",
    "#         preds_denormalized = preds\n",
    "\n",
    "#         sum_test_r2 = sum_test_r2 + r2_score(y_denormalized,preds_denormalized)\n",
    "#         y_test1.append(preds_denormalized.numpy().reshape(-1,1))\n",
    "#         y_test2.append(y_denormalized.numpy().reshape(-1,1))\n",
    "        \n",
    "#     sum_train_r2 = sum_train_r2 / len(train_dataloader) #数据存到cpu= 直接用.item()也行\n",
    "#     sum_test_r2 = sum_test_r2 / len(test_dataloader)\n",
    "\n",
    "# print(sum_train_r2)\n",
    "# print(sum_test_r2)\n",
    "# np.savetxt(save_path + '//' + 'Y_predict.csv',\n",
    "#            np.hstack((np.vstack((np.array(y_train1).reshape(-1,1),np.array(y_test1).reshape(-1,1))),\n",
    "#                       np.vstack((np.array(y_train2).reshape(-1,1),np.array(y_test2).reshape(-1,1))))),\n",
    "#            delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: [0/20]: : 24it [00:40,  1.68s/it]                    \n",
      "epoch: [1/20]: : 24it [00:41,  1.73s/it]                    \n",
      "epoch: [2/20]: : 24it [00:41,  1.74s/it]                    \n",
      "epoch: [3/20]: : 24it [00:42,  1.75s/it]                    \n",
      "epoch: [4/20]: : 24it [00:41,  1.73s/it]                    \n",
      "epoch: [5/20]: : 24it [00:41,  1.74s/it]                    \n",
      "epoch: [6/20]: : 24it [00:41,  1.73s/it]                    \n",
      "epoch: [7/20]: : 24it [00:41,  1.74s/it]                    \n",
      "epoch: [8/20]: : 24it [00:41,  1.75s/it]                    \n",
      "epoch: [9/20]: : 24it [00:42,  1.75s/it]                    \n",
      "epoch: [10/20]: : 24it [00:42,  1.75s/it]                    \n",
      "epoch: [11/20]: : 24it [00:41,  1.74s/it]                    \n",
      "epoch: [12/20]: : 24it [00:41,  1.74s/it]                    \n",
      "epoch: [13/20]: : 24it [00:41,  1.75s/it]                    \n",
      "epoch: [14/20]: : 24it [00:41,  1.75s/it]                    \n",
      "epoch: [15/20]: : 24it [00:41,  1.73s/it]                    \n",
      "epoch: [16/20]: : 24it [00:41,  1.74s/it]                    \n",
      "epoch: [17/20]: : 24it [00:41,  1.74s/it]                    \n",
      "epoch: [18/20]: : 24it [00:42,  1.75s/it]                    \n",
      "epoch: [19/20]: : 24it [00:42,  1.75s/it]                    \n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "perturb.cpu()\n",
    "another_epoch = int(num_epochs/40)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(train_data, train_label), batch_size=150, shuffle=False)\n",
    "\n",
    "for epochs in range(another_epoch):\n",
    "    loop = tqdm(train_dataloader, total=len(test_dataloader))\n",
    "    for x, y in loop:\n",
    "        x = x.cpu()\n",
    "        pp,pm,sigma = perturb(x)\n",
    "        sigma = sigma.cpu().detach()#记得换个位置存\n",
    "        p = model(x)\n",
    "        pp = model(pp)\n",
    "        pm = model(pm)\n",
    "        pp = torch.abs(pp - p)\n",
    "        pm = torch.abs(pm - p)\n",
    "        const = torch.full([pp.size()[0],1],torch.max(p).item())\n",
    "        const_0 = torch.full([pp.size()[0],1],torch.min(p).item())\n",
    "\n",
    "        l1 = criterion(pp,const_0)\n",
    "        l2 = criterion(pm,const)\n",
    "        l3 = criterion(sigma,torch.zeros(4096))\n",
    "\n",
    "        loss = l1 + l2 + l3\n",
    "\n",
    "\n",
    "        perturb_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        perturb_optimizer.step()\n",
    "        \n",
    "        loop.set_description('epoch: [%d/%d]' % (epochs, another_epoch))\n",
    "\n",
    "    if epochs % step_epoch == 0: # 每step_epoch次epoch存一次数据\n",
    "        save_sigma = sigma.clone().numpy()\n",
    "        all_sigma.append(save_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# save_sigma = scaler.fit_transform(save_sigma.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "a = np.array(all_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25589982, 0.56894714, 0.8381044 , ..., 0.4142936 , 0.2013672 ,\n",
       "       0.48210844], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([676.69], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_wl[np.argmax(save_sigma)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.savetxt(save_path + '//' + 'sigma11.csv',np.vstack((raw_wl.T,a)).T,delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "aa = np.std(a,axis=0)\n",
    "scaler = MinMaxScaler()\n",
    "aa = scaler.fit_transform(aa.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.savetxt(save_path + '//' + 'aa.csv',aa,delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.savetxt(save_path + '//' + 'sigma.csv',all_sigma[-1],delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_sigma.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_wl\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_sigma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m,fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\shape_base.py:370\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "np.savetxt(save_path + '//' + 'save_sigma.csv',np.hstack((raw_wl,save_sigma)),delimiter=',',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "x = np.linspace(0,100,len(all_sigma[0]))\n",
    "plt.plot(x,all_sigma[0])\n",
    "# plt.plot(x,all_sigma[-1],c = 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zjy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
